{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ee6cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import gdown\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203a5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdown.download(f'https://drive.google.com/uc?id=1hHS3oUW7H1KdC341LeYAsroaJiWWcnL7', \"politifact_data.csv\", quiet=True)\n",
    "shift_df = pd.read_csv('data/politifact_data.csv')[['media', 'article', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23865320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 for drift, 0 for non-drift\n",
    "def sentiment_score(result):\n",
    "    numerical_scores = [scale[sentiment['label']] * sentiment['score'] for sentiment in result[0]]\n",
    "    overall_score = sum(numerical_scores)\n",
    "    return overall_score\n",
    "\n",
    "def sentiment_shift(article):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    distilled_student_sentiment_classifier = pipeline(\n",
    "        model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", \n",
    "        return_all_scores=True,\n",
    "        device=device\n",
    "    )\n",
    "    cleaned_text = re.sub(r'\\xa0', ' ', article)\n",
    "    cleaned_text = re.sub(r'\\\\', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\n', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replace multiple spaces with a single space\n",
    "    cleaned_text = cleaned_text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    cleaned_text = re.sub(r'“|”', '\"', cleaned_text)\n",
    "    scale = {\n",
    "        'positive' : 1,\n",
    "        'neutral' : 0,\n",
    "        'negative' : -1\n",
    "    }\n",
    "    data = []\n",
    "    sentences = sent_tokenize(article)\n",
    "    for sentence in sentences:\n",
    "        # For now, trim sentence if longer than 512\n",
    "        if len(sentence) > 512:\n",
    "            sentence = sentence[:512]\n",
    "        result = sentiment_score(distilled_student_sentiment_classifier(sentence))\n",
    "        data.append(result)\n",
    "    alpha = 0.05\n",
    "    half = len(data)//2\n",
    "    first_half = data[:half]\n",
    "    second_half = data[half:]\n",
    "    t_statistic, p_value = ttest_ind(first_half, second_half)\n",
    "    if p_value < alpha:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4226d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 for drift, 0 for non-drift\n",
    "def topic_shift(article):\n",
    "    cleaned_text = re.sub(r'\\xa0', ' ', article)\n",
    "    cleaned_text = re.sub(r'\\\\', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\n', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replace multiple spaces with a single space\n",
    "    cleaned_text = cleaned_text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    cleaned_text = re.sub(r'“|”', '\"', cleaned_text)\n",
    "    scale = {\n",
    "        'positive' : 1,\n",
    "        'neutral' : 0,\n",
    "        'negative' : -1\n",
    "    }\n",
    "    data = []\n",
    "    sentences = sent_tokenize(article)\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(sentences)\n",
    "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "    topic_distribution = lda.transform(dtm)\n",
    "    dominant_topic_per_document = topic_distribution.argmax(axis=1)\n",
    "    half = len(dominant_topic_per_document) // 2\n",
    "    epsilon = 1e-9\n",
    "    first_half = dominant_topic_per_document[:half]\n",
    "    second_half = dominant_topic_per_document[half:]\n",
    "    min_value = min(min(first_half), min(second_half))\n",
    "    max_value = max(max(first_half), max(second_half))\n",
    "    histogram1, _ = np.histogram(first_half, bins=np.arange(min_value, max_value + 2))\n",
    "    histogram2, _ = np.histogram(second_half, bins=np.arange(min_value, max_value + 2))\n",
    "    contingency_table = np.array([histogram1, histogram2]) + epsilon\n",
    "    _, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    if p_value < 0.05:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fcedd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import sent_tokenize, pos_tag, ne_chunk\n",
    "def perform_ner(text):\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    named_entities = ne_chunk(pos_tags)\n",
    "    return [entity for entity in named_entities if isinstance(entity, nltk.Tree)]\n",
    "\n",
    "def tree_to_string(tree):\n",
    "    if isinstance(tree, nltk.Tree):\n",
    "        return ' '.join([tree_to_string(child) for child in tree])\n",
    "    else:\n",
    "        return tree[0]\n",
    "def ner_shift(article):\n",
    "    def clean_text(text):\n",
    "        cleaned_text = re.sub(r'\\xa0', ' ', text)\n",
    "        cleaned_text = re.sub(r'\\\\', '', cleaned_text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replace multiple spaces with a single space\n",
    "        cleaned_text = cleaned_text.encode('ascii', 'ignore').decode('utf-8')\n",
    "        cleaned_text = cleaned_text.strip()\n",
    "        cleaned_text = re.sub(r'“|”', '\"', cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    sentences = sent_tokenize(article)\n",
    "    sentences_length = len(sentences)\n",
    "    half_index = sentences_length // 2\n",
    "    first_half = ' '.join(sentences[:half_index])\n",
    "    second_half = ' '.join(sentences[half_index:])\n",
    "    cleaned_first_half = clean_text(first_half)\n",
    "    cleaned_second_half = clean_text(second_half)\n",
    "    entities_first_half = [tree_to_string(entity) for entity in perform_ner(cleaned_first_half)]\n",
    "    entities_second_half = [tree_to_string(entity) for entity in perform_ner(cleaned_second_half)]\n",
    "    ner_shift_count = len(set(entities_second_half) - set(entities_first_half))\n",
    "    return ner_shift_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e780fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5569/2648491459.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msent_shift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msent_shift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5569/635093789.py\u001b[0m in \u001b[0;36msentiment_shift\u001b[0;34m(article)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msentiment_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     distilled_student_sentiment_classifier = pipeline(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "shift_df['topic_drift'] = shift_df['article'].apply(topic_shift)\n",
    "sent_shift = []\n",
    "for i in range(shift_df.shape[0]):\n",
    "    result = sentiment_shift(shift_df.iloc[i]['article'])\n",
    "    sent_shift.append(result)\n",
    "    if i % 100 == 0:\n",
    "        print(f'Iteration',i,'is done')\n",
    "        clear_output(wait=True)\n",
    "shift_df['sentiment_drift'] = pd.Series(sent_shift)\n",
    "shift_df['ner_shift_count'] = shift_df['article'].apply(ner_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799058c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift_df.to_csv('context_shift_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf77970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
