{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb328507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import openai\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1419560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_author_info(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        score_elements = soup.find_all(\"div\", class_=\"m-scorecard__item\")\n",
    "        percentages = []\n",
    "        check_nums = []\n",
    "        for element in score_elements:\n",
    "            percentage = element.find('span', class_='m-scorecard__value').find('noscript').text\n",
    "            check_num = int(element.find('p', class_='m-scorecard__checks').find('a').text.split(' ')[0])\n",
    "            percentages.append(percentage)\n",
    "            check_nums.append(check_num)\n",
    "        return percentages, check_nums\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b13b51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_content_info(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        try:\n",
    "            lists = soup.find(\"div\", class_=\"short-on-time\").find_all('li')\n",
    "            summaries = []\n",
    "            for bullet_point in lists:\n",
    "                summary = bullet_point.text.strip('\\n')\n",
    "                summaries.append(summary)\n",
    "        except:\n",
    "            summaries = []\n",
    "        \n",
    "        \n",
    "        article = soup.find('article', class_='m-textblock').text.replace('\\n', '').replace('\\t', '')\n",
    "        return summaries, article\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2ca0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_filter(tag):\n",
    "    return tag.name == \"a\" and tag.get('href', '').startswith('/factchecks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6878ee63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 663 is done.\n",
      "Page 664 is done.\n",
      "Page 665 is done.\n",
      "Page 666 is done.\n",
      "Page 667 is done.\n",
      "Page 668 is done.\n",
      "Page 669 is done.\n",
      "Page 670 is done.\n",
      "Page 671 is done.\n",
      "Page 672 is done.\n",
      "Page 673 is done.\n",
      "Page 674 is done.\n",
      "Page 675 is done.\n",
      "Page 676 is done.\n",
      "Page 677 is done.\n",
      "Page 678 is done.\n",
      "Page 679 is done.\n",
      "Page 680 is done.\n",
      "Page 681 is done.\n",
      "Page 682 is done.\n",
      "Page 683 is done.\n",
      "Page 684 is done.\n",
      "Page 685 is done.\n",
      "Page 686 is done.\n",
      "Page 687 is done.\n",
      "Page 688 is done.\n",
      "Page 689 is done.\n",
      "Page 690 is done.\n",
      "Page 691 is done.\n",
      "Page 692 is done.\n",
      "Page 693 is done.\n",
      "Page 694 is done.\n",
      "Page 695 is done.\n",
      "Page 696 is done.\n",
      "Page 697 is done.\n",
      "Page 698 is done.\n",
      "Page 699 is done.\n",
      "Page 700 is done.\n",
      "Page 701 is done.\n",
      "Page 702 is done.\n",
      "Page 703 is done.\n",
      "Page 704 is done.\n",
      "Page 705 is done.\n",
      "Page 706 is done.\n",
      "Page 707 is done.\n",
      "Page 708 is done.\n",
      "Page 709 is done.\n",
      "Page 710 is done.\n",
      "Page 711 is done.\n",
      "Page 712 is done.\n",
      "Page 713 is done.\n",
      "Page 714 is done.\n",
      "Page 715 is done.\n",
      "Page 716 is done.\n",
      "Page 717 is done.\n",
      "Page 718 is done.\n",
      "Page 719 is done.\n",
      "Page 720 is done.\n",
      "Page 721 is done.\n",
      "Page 722 is done.\n",
      "Page 723 is done.\n",
      "Page 724 is done.\n",
      "Page 725 is done.\n",
      "Page 726 is done.\n",
      "Page 727 is done.\n",
      "Page 728 is done.\n",
      "Page 729 is done.\n",
      "Page 730 is done.\n",
      "Page 731 is done.\n",
      "Page 732 is done.\n",
      "Page 733 is done.\n",
      "Page 734 is done.\n",
      "Page 735 is done.\n",
      "Page 736 is done.\n",
      "Page 737 is done.\n",
      "Page 738 is done.\n",
      "Page 739 is done.\n",
      "Page 740 is done.\n",
      "Page 741 is done.\n",
      "Page 742 is done.\n",
      "Page 743 is done.\n",
      "Page 744 is done.\n",
      "Page 745 is done.\n",
      "Page 746 is done.\n",
      "Page 747 is done.\n",
      "Page 748 is done.\n",
      "Page 749 is done.\n",
      "Page 750 is done.\n",
      "Page 751 is done.\n",
      "Page 752 is done.\n",
      "Page 753 is done.\n",
      "Page 754 is done.\n",
      "Page 755 is done.\n",
      "Page 756 is done.\n",
      "Page 757 is done.\n",
      "Page 758 is done.\n",
      "Page 759 is done.\n",
      "Page 760 is done.\n",
      "Page 761 is done.\n",
      "Page 762 is done.\n",
      "Page 763 is done.\n",
      "Page 764 is done.\n",
      "Page 765 is done.\n",
      "Page 766 is done.\n",
      "Page 767 is done.\n",
      "Page 768 is done.\n",
      "Page 769 is done.\n",
      "Page 770 is done.\n",
      "Page 771 is done.\n",
      "Page 772 is done.\n",
      "Page 773 is done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_135/3691535606.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  poli_df = pd.DataFrame(np.array(info_list), columns = ['media', 'when/where', 'content', 'label', 'speaker', 'documented_time', 'percentages', 'check_nums', 'summaries', 'article'])\n"
     ]
    }
   ],
   "source": [
    "# The current number of pages\n",
    "pages = 0\n",
    "info_list = []\n",
    "url_org = 'https://www.politifact.com'\n",
    "\n",
    "for page_id in range(0, 2000):\n",
    "    time.sleep(3)\n",
    "    url = f\"https://www.politifact.com/factchecks/list/?page={page_id + 1}&\"\n",
    "    response = requests.get(url)\n",
    "    try:\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_elements = soup.find_all(\"li\", class_=\"o-listicle__item\")\n",
    "            for article in article_elements:\n",
    "                media = article.find(\"a\", class_=\"m-statement__name\").text.strip('\\n')\n",
    "                when_where = article.find(\"div\", class_=\"m-statement__desc\").text.strip('\\n')\n",
    "                content = article.find_all(custom_filter)[0].text.strip('\\n')\n",
    "                credibility = article.find_all(\"img\")[-1].get('alt')\n",
    "                author = article.find(\"footer\", class_=\"m-statement__footer\").text.strip('\\n').split('•')[0].strip('By ')\n",
    "                documented_time = article.find(\"footer\", class_=\"m-statement__footer\").text.strip('\\n').split('•')[1]                \n",
    "                author_url = url_org + article.find('div', class_='m-statement__meta').find('a').get('href')\n",
    "                content_url = url_org + article.find('div', class_='m-statement__quote').find('a').get('href')\n",
    "                percentages, check_nums = fetch_author_info(author_url)\n",
    "                percentages = np.array(percentages)\n",
    "                check_nums = np.array(check_nums)\n",
    "                summaries, article = fetch_content_info(content_url)\n",
    "                summaries = np.array(summaries)\n",
    "                info_list.append([media, when_where, content, credibility, author, documented_time, percentages, check_nums, summaries, article])      \n",
    "            print(f'Page {page_id + 1} is done.')\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")  \n",
    "    except:\n",
    "        print(f'Failed at this page')\n",
    "        poli_df = pd.DataFrame(np.array(info_list), columns = ['media', 'when/where', 'content', 'label', 'speaker', 'documented_time', 'percentages', 'check_nums', 'summaries', 'article'])\n",
    "        poli_df.to_csv('../../data/politifact_data.csv')\n",
    "        break\n",
    "poli_df = pd.DataFrame(np.array(info_list), columns = ['media', 'when/where', 'content', 'label', 'speaker', 'documented_time', 'percentages', 'check_nums', 'summaries', 'article'])\n",
    "poli_df.to_csv('../../data/politifact_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
