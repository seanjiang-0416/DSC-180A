{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c0583c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024cca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = ['True', 'Mostly True ','Half True ', 'Mostly False', 'False', 'Pants on Fire']\n",
    "score = np.array([10, 8, 6, 4, 2, 0])\n",
    "poli_df = pd.read_csv('politifact_data.csv')\n",
    "def transform_percentages(col):\n",
    "    return sum(np.array(ast.literal_eval(col.replace('%', '').replace('\\'', '').replace(' ', ',')))/100 * score)\n",
    "poli_df['credibility_score'] = poli_df['percentages'].apply(transform_percentages)\n",
    "poli_df['content'] = poli_df['content'].str.replace(\"“\", \"\").str.replace(\"”\",\"\").str.replace(\"\\\"\", '')\n",
    "poli_df = poli_df.rename(columns = {'speaker':'recorder', 'media':'source'})\n",
    "\n",
    "def extract_date(text):\n",
    "    date_pattern = re.compile(r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{1,2},\\s\\d{4}\\b')\n",
    "    match = date_pattern.search(text) \n",
    "    if match:\n",
    "        date = match.group()\n",
    "        return date\n",
    "poli_df['publish_time'] = poli_df['when/where'].apply(extract_date)\n",
    "poli_df['content'].head()[0]\n",
    "clean_df = poli_df[['source', 'credibility_score']].drop_duplicates().reset_index().drop(columns = 'index')\n",
    "clean_df.head()\n",
    "import requests\n",
    "def search_wikipedia(query, num_results=15):\n",
    "    endpoint_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'list': 'search',\n",
    "        'srsearch': query,\n",
    "        'srlimit': num_results\n",
    "    }\n",
    "    response = requests.get(endpoint_url, params=params)\n",
    "    data = response.json()\n",
    "    output = []\n",
    "    search_results = data['query']['search']\n",
    "    for result in search_results:\n",
    "        title = result['title']\n",
    "        output.append(title)\n",
    "    return \" \".join(output)\n",
    "def scrape_titles(col):\n",
    "    return search_wikipedia(col, num_results=20)\n",
    "clean_df['source_history'] = clean_df['source'].apply(scrape_titles)\n",
    "# clean_df.to_csv('credibility.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae4d777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv('credibility.csv', index_col=None).drop(columns = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f4d46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>credibility_score</th>\n",
       "      <th>source_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Instagram List of most-followed Instagram acco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>5.14</td>\n",
       "      <td>Scott Walker Scott Walker (politician) Scott W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Viral image</td>\n",
       "      <td>1.60</td>\n",
       "      <td>Viral phenomenon Imgur The dress Midjourney Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>1.86</td>\n",
       "      <td>Facebook Facebook 3D Posts Meta Platforms Hist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New York Citizens Audit</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Sovereign citizen movement KPMG BNY Mellon Arc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    source  credibility_score  \\\n",
       "0          Instagram posts               1.74   \n",
       "1             Scott Walker               5.14   \n",
       "2              Viral image               1.60   \n",
       "3           Facebook posts               1.86   \n",
       "4  New York Citizens Audit               2.00   \n",
       "\n",
       "                                      source_history  \n",
       "0  Instagram List of most-followed Instagram acco...  \n",
       "1  Scott Walker Scott Walker (politician) Scott W...  \n",
       "2  Viral phenomenon Imgur The dress Midjourney Da...  \n",
       "3  Facebook Facebook 3D Posts Meta Platforms Hist...  \n",
       "4  Sovereign citizen movement KPMG BNY Mellon Arc...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "383b4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a84c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92cf2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df[clean_df['source_history'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c09fcd97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
    "    tokens = nltk.word_tokenize(text)    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "clean_df['preprocessed_source'] = clean_df['source_history'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44bc4d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for LinearRegression: 14.271328247245256\n",
      "Mean Squared Error for Ridge: 8.287456497384378\n",
      "Mean Squared Error for Lasso: 8.317311604743733\n",
      "Mean Squared Error for RandomForestRegressor: 7.911604973033622\n",
      "Mean Squared Error for BayesianRidge: 7.856691318441045\n"
     ]
    }
   ],
   "source": [
    "X = clean_df['preprocessed_source']\n",
    "y = clean_df['credibility_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(),\n",
    "    Lasso(),\n",
    "    RandomForestRegressor(),\n",
    "#     make_pipeline(PolynomialFeatures(degree=2), LinearRegression()),\n",
    "    BayesianRidge(),\n",
    "]\n",
    "for model in models:\n",
    "    if model.__class__.__name__ != 'BayesianRidge':\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "    else:    \n",
    "        model.fit(X_train_tfidf.toarray(), y_train)\n",
    "        y_pred = model.predict(X_test_tfidf.toarray())\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error for {model.__class__.__name__}: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23340053",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = clean_df['source_history'][[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1fadf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e3f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
