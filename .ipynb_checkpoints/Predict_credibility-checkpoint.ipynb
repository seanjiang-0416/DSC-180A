{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0583c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024cca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = ['True', 'Mostly True ','Half True ', 'Mostly False', 'False', 'Pants on Fire']\n",
    "score = np.array([10, 8, 6, 4, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691a1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_df = pd.read_csv('politifact_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00af5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_percentages(col):\n",
    "    return sum(np.array(ast.literal_eval(col.replace('%', '').replace('\\'', '').replace(' ', ',')))/100 * score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f987db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_df['credibility_score'] = poli_df['percentages'].apply(transform_percentages)\n",
    "poli_df['content'] = poli_df['content'].str.replace(\"“\", \"\").str.replace(\"”\",\"\").str.replace(\"\\\"\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae28eae9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poli_df = poli_df.rename(columns = {'speaker':'recorder', 'media':'source'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6d6e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_date(text):\n",
    "    date_pattern = re.compile(r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{1,2},\\s\\d{4}\\b')\n",
    "    match = date_pattern.search(text) \n",
    "    if match:\n",
    "        date = match.group()\n",
    "        return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c1c3b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Haaretz investigation reveals discrepancies in Israel’s reporting on October 7th death toll.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poli_df['publish_time'] = poli_df['when/where'].apply(extract_date)\n",
    "poli_df['content'].head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d9bb321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>credibility_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>5.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Viral image</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>1.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New York Citizens Audit</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    source  credibility_score\n",
       "0          Instagram posts               1.74\n",
       "1             Scott Walker               5.14\n",
       "2              Viral image               1.60\n",
       "3           Facebook posts               1.86\n",
       "4  New York Citizens Audit               2.00"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = poli_df[['source', 'credibility_score']].drop_duplicates().reset_index().drop(columns = 'index')\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c50a44c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Could take out \"post\", but want to keep it as an experiment\n",
    "sum(clean_df['source'].str.contains('post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbd29eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_KEY = 'AIzaSyCusVqc2nRP4KyEsh3E8ALdfdae3nkeMk4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961be17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "baaf6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def search_wikipedia(query, num_results=15):\n",
    "    endpoint_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'list': 'search',\n",
    "        'srsearch': query,\n",
    "        'srlimit': num_results\n",
    "    }\n",
    "    response = requests.get(endpoint_url, params=params)\n",
    "    data = response.json()\n",
    "    output = []\n",
    "    search_results = data['query']['search']\n",
    "    for result in search_results:\n",
    "        title = result['title']\n",
    "        output.append(title)\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53fa867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_titles(col):\n",
    "    return search_wikipedia(col, num_results=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dc6b6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['source_history'] = clean_df['source'].apply(scrape_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba6e5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df.to_csv('credibility.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "383b4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a84c09a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zhj003/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/zhj003/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/zhj003/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c09fcd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
    "    tokens = nltk.word_tokenize(text)    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "clean_df['preprocessed_source'] = clean_df['source_history'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc4d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for LinearRegression: 14.801730169917212\n",
      "Mean Squared Error for Ridge: 8.209604876330653\n",
      "Mean Squared Error for Lasso: 8.223176898709822\n"
     ]
    }
   ],
   "source": [
    "X = clean_df['preprocessed_source']\n",
    "y = clean_df['credibility_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(),\n",
    "    Lasso(),\n",
    "    RandomForestRegressor(),\n",
    "    make_pipeline(PolynomialFeatures(degree=2), LinearRegression()),\n",
    "    BayesianRidge(),\n",
    "]\n",
    "for model in models:\n",
    "    if model.__class__.__name__ != 'BayesianRidge':\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "    else:    \n",
    "        model.fit(X_train_tfidf.toarray(), y_train)\n",
    "        y_pred = model.predict(X_test_tfidf.toarray())\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error for {model.__class__.__name__}: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "23340053",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = clean_df['source_history'][[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b1fadf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "88e3f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f874a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
