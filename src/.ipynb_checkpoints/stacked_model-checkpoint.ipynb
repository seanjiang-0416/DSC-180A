{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b0cdd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185cd43",
   "metadata": {},
   "source": [
    "## Stacked Model\n",
    "This notebook includes a combination of three factors -- credibility, political affiliation, and context veracity. Each step can be refered to its experimental notebook located in the notebook file from the root. The output contains a GridSearchCV and the best accuracy model and its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9bd68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 20:43:49.071445: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Input is a Series/np.array\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "def text_embedding(data):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "    \n",
    "    def get_bert_embeddings(data):\n",
    "        tokens = tokenizer(data.tolist(), padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            embeddings = bert_model(**tokens).last_hidden_state.mean(dim=1)\n",
    "        return embeddings\n",
    "\n",
    "    batch_size = 128\n",
    "    num_samples = len(data)\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    embeddings_list = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        batch_data = data.iloc[start_idx:end_idx]\n",
    "        batch_embeddings = get_bert_embeddings(batch_data)\n",
    "        embeddings_list.append(batch_embeddings)\n",
    "\n",
    "    embeddings = torch.cat(embeddings_list, dim=0).cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c38d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for RandomForestRegressor: 8.200894296021652\n"
     ]
    }
   ],
   "source": [
    "# Train credibility and reliability, for details, please refer to notebook/experimental_notebooks/credibility_reliability.ipynb\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def get_username(instagram_url):\n",
    "    parsed_url = urlparse(instagram_url)\n",
    "    path_segments = parsed_url.path.strip('/').split('/')\n",
    "    username = path_segments[-1]\n",
    "    return username\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
    "    tokens = nltk.word_tokenize(text)    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "# reliability.csv contains social media user information\n",
    "insta_df = pd.read_csv('../data/reliability.csv').drop(columns = 'Unnamed: 0').drop_duplicates(subset='username', keep='last')\n",
    "social_media = pd.read_csv('../data/credibility_with_insta_username.csv').drop(columns = 'Unnamed: 0')\n",
    "cleaned_social_media = social_media[social_media['instagram_uri'].str.contains('instagram', na=False)].reset_index().drop(columns = 'index')\n",
    "cleaned_social_media['username'] = cleaned_social_media['instagram_uri'].apply(get_username)\n",
    "unique_df = cleaned_social_media[['username', 'source']].drop_duplicates(subset='source', keep='last')\n",
    "merged_df = pd.merge(insta_df, unique_df, on='username', how='inner').reset_index().drop(columns = 'index')\n",
    "clean_df = pd.read_csv('../data/credibility.csv', index_col=None).drop(columns = 'Unnamed: 0')\n",
    "cred_reliability = pd.merge(merged_df, clean_df, on='source', how='inner').reset_index().drop(columns='index')\n",
    "cred_reliability['followees_to_followers_ratio'] = cred_reliability['followees'] / cred_reliability['followers'] + 1e-10\n",
    "cred_reliability = cred_reliability[['source', 'source_history', 'mediacount', 'followees_to_followers_ratio', 'is_verified', 'credibility_score']]\n",
    "cred_reliability = cred_reliability.dropna(subset=['source_history'])\n",
    "cred_reliability = cred_reliability.replace([np.inf, -np.inf], np.nan)\n",
    "cred_reliability = cred_reliability.dropna(subset=['followees_to_followers_ratio'])\n",
    "cred_reliability['is_verified'] = cred_reliability['is_verified'].apply(lambda x: 0 if x else 1)\n",
    "cred_reliability['preprocessed_source'] = cred_reliability['source_history'].apply(preprocess_text)\n",
    "X = cred_reliability['preprocessed_source']\n",
    "X = text_embedding(X)\n",
    "X = np.concatenate((X[:, :50], cred_reliability[['mediacount', 'followees_to_followers_ratio', 'is_verified']].values), axis=1)\n",
    "y = cred_reliability['credibility_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Previously validated Random Forrest has the best performance\n",
    "cred_reliability_model = RandomForestRegressor()\n",
    "cred_reliability_model.fit(X_train, y_train)\n",
    "y_pred = cred_reliability_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error for {cred_reliability_model.__class__.__name__}: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62145fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8177770067083044\n"
     ]
    }
   ],
   "source": [
    "# Political Affiliation, for details, please refer to notebook/experimental_notebooks/political_affiliation_binary_classification.ipynb\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tweet_df = pd.read_csv('../data/tweets_dataset.csv')\n",
    "tweet_df_clean = tweet_df.drop(columns = 'Handle')\n",
    "tweet_df_clean['Party'] = tweet_df_clean['Party'].apply(lambda x: 0 if x == 'Democrat' else 1)\n",
    "tweet_df_clean['Tweet'] = tweet_df_clean['Tweet'].str.lower() \n",
    "tweet_df_clean['Tweet'] = tweet_df_clean['Tweet'].str.replace(r'http\\S+', '') \n",
    "tweet_df_clean['Tweet'] = tweet_df_clean['Tweet'].str.replace(r'@\\w+', '')\n",
    "tweet_df_clean['Tweet'] = tweet_df_clean['Tweet'].str.replace(r'#\\w+', '')\n",
    "tweet_df_clean['Tweet'] = tweet_df_clean['Tweet'].str.replace(r'[^a-zA-Z\\s]', '') \n",
    "stop_words = set(stopwords.words('english'))\n",
    "tweet_df_clean['Tweet'] = tweet_df_clean['Tweet'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweet_df_clean['Tweet'], tweet_df_clean['Party'], test_size=0.2, random_state=42)\n",
    "# Generate 1-gram to 3-gram to create more features\n",
    "poli_vectorizer = CountVectorizer(ngram_range=(1, 3)) \n",
    "X_train_vectorized = poli_vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = poli_vectorizer.transform(X_test)\n",
    "\n",
    "# Previously validated MultinomialNB has the best performance\n",
    "poli_affili_model = MultinomialNB()\n",
    "poli_affili_model.fit(X_train_vectorized, y_train)\n",
    "y_pred = poli_affili_model.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b77534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Veracity\n",
    "# For details about this preprocessed dataset, please refer to notebook/experimental_notebooks/context_veracity.ipynb\n",
    "gdown.download(f\"https://drive.google.com/uc?id=16ohLi5nbC0yYk6oNou8YArLQY2cxnQWm\", \"../data/context_shift_score.csv\", quiet=True)\n",
    "shift_df = pd.read_csv('../data/context_shift_score.csv').drop(columns = 'Unnamed: 0')\n",
    "def calculate_contextual_drift(row):\n",
    "    a = 0.4  # coefficient for topic_drift\n",
    "    b = 0.4  # coefficient for sentiment_drift\n",
    "    c = 0.1  # coefficient for ner_shift_count\n",
    "    d = 0.1  # constant term\n",
    "    return a * row['topic_drift'] + b * row['sentiment_drift'] + c * row['ner_shift_count'] + d\n",
    "shift_df['contextual_drift'] = shift_df.apply(calculate_contextual_drift, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b52b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the models and dataframe, predict on politifact\n",
    "cred_reliability_no_label = cred_reliability.drop(columns = 'credibility_score')\n",
    "merged_df = pd.merge(shift_df, cred_reliability_no_label, left_on='media', right_on='source', how='inner').reset_index().drop(columns='index')\n",
    "label_mapping = {\n",
    "    'false': 0,\n",
    "    'barely-true': 1,\n",
    "    'full-flop': 2,\n",
    "    'half-flip': 3,\n",
    "    'half-true': 4,\n",
    "    'mostly-true': 5,\n",
    "    'no-flip': 6,\n",
    "    'pants-fire': 7,\n",
    "    'true': 8\n",
    "}\n",
    "def convert_label(label):\n",
    "    return label_mapping[label]\n",
    "merged_df['label'] = merged_df['label'].apply(convert_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80a17d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict political affiliation from article\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
    "    tokens = nltk.word_tokenize(text)    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "merged_df['article'] = merged_df['article'].apply(preprocess_text)\n",
    "transformed_article = poli_vectorizer.transform(merged_df['article'])\n",
    "merged_df['poli_affiliation'] = poli_affili_model.predict(transformed_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2b3bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Credibility-Reliability \n",
    "transformed_source = text_embedding(merged_df['preprocessed_source'])[:, :50]\n",
    "combined_features = np.concatenate((transformed_source, merged_df[['mediacount', 'followees_to_followers_ratio', 'is_verified']].values), axis=1)\n",
    "merged_df['cred_reliability'] = cred_reliability_model.predict(combined_features)\n",
    "result_df = merged_df[['contextual_drift', 'poli_affiliation', 'cred_reliability', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d389690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Decision Tree\n",
      "max_depth: None\n",
      "Accuracy: 0.3327917682101273\n",
      "==============================\n",
      "max_depth: 5\n",
      "Accuracy: 0.3254806390468454\n",
      "==============================\n",
      "max_depth: 10\n",
      "Accuracy: 0.3254806390468454\n",
      "==============================\n",
      "max_depth: 15\n",
      "Accuracy: 0.3279176821012727\n",
      "==============================\n",
      "Classifier: Random Forest\n",
      "n_estimators: 50\n",
      "Accuracy: 0.34524776604386676\n",
      "==============================\n",
      "n_estimators: 100\n",
      "Accuracy: 0.3422691578662334\n",
      "==============================\n",
      "n_estimators: 200\n",
      "Accuracy: 0.346872461413485\n",
      "==============================\n",
      "max_depth: None\n",
      "Accuracy: 0.346872461413485\n",
      "==============================\n",
      "max_depth: 5\n",
      "Accuracy: 0.3249390739236393\n",
      "==============================\n",
      "max_depth: 10\n",
      "Accuracy: 0.33549959382615757\n",
      "==============================\n",
      "Classifier: Logistic Regression\n",
      "C: 0.001\n",
      "Accuracy: 0.30354725155699974\n",
      "==============================\n",
      "C: 0.01\n",
      "Accuracy: 0.3051719469266179\n",
      "==============================\n",
      "C: 0.1\n",
      "Accuracy: 0.3108583807202816\n",
      "==============================\n",
      "C: 1\n",
      "Accuracy: 0.31112916328188467\n",
      "==============================\n",
      "C: 10\n",
      "Accuracy: 0.3108583807202816\n",
      "==============================\n",
      "Best Model: Random Forest\n",
      "Best Accuracy: 0.346872461413485\n",
      "Best Hyperparameters: {'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = result_df[['contextual_drift', 'poli_affiliation', 'cred_reliability']]\n",
    "y = result_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifiers = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=10000)\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'Decision Tree': {'max_depth': [None, 5, 10, 15]},\n",
    "    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [None, 5, 10]},\n",
    "    'Logistic Regression': {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "}\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"Classifier: {clf_name}\")\n",
    "    \n",
    "    # If classifier has hyperparameters, perform nested loop for hyperparameter tuning\n",
    "    if clf_name in param_grids:\n",
    "        for param_name, param_values in param_grids[clf_name].items():\n",
    "            for param_value in param_values:\n",
    "                # Set the hyperparameter value and fit the model\n",
    "                setattr(clf, param_name, param_value)\n",
    "                clf.fit(X_train, y_train)\n",
    "                \n",
    "                # Make predictions and evaluate the model\n",
    "                y_pred = clf.predict(X_test)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                \n",
    "                # Check if the current set of hyperparameters achieved a better accuracy\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_model = clf_name\n",
    "                    best_params = {param_name: param_value}\n",
    "                \n",
    "                print(f\"{param_name}: {param_value}\")\n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "                print(\"=\"*30)\n",
    "    \n",
    "    # If no hyperparameters, fit the model directly\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Check if the current model achieved a better accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = clf_name\n",
    "            best_params = None\n",
    "            \n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(\"=\"*30)\n",
    "\n",
    "# Print the best results\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Best Accuracy: {best_accuracy}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99cc9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
